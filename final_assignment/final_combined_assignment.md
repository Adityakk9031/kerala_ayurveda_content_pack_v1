# Kerala Ayurveda Agentic AI Assignment - Final Submission

**Author:** [Generated by Cursor AI Assistant]  
**Date:** [Current Date]  
**Corpus:** Kerala Ayurveda Content Pack v1

---

# Table of Contents

1. [Part A: RAG Design](#part-a-rag-design)
2. [Part A: Code and Examples](#part-a-code-and-examples)
3. [Part B: Agentic Workflow & Evaluation](#part-b-agentic-workflow--evaluation)
4. [Reflection](#reflection)

---

# Part A: RAG Design

## 1. Chunking Plan

### Document Types and Chunking Strategy

**Markdown Documents (Foundational & Product Dossiers):**
- **Chunk size:** 512 tokens (approximately 400-450 words)
- **Overlap:** 128 tokens (approximately 100 words) between adjacent chunks
- **Special handling:**
  - Preserve heading hierarchy (H1, H2, H3) as metadata
  - Each chunk includes its parent heading path (e.g., "Ayurveda Foundations > The Tridosha Model > Vata")
  - Split at natural boundaries (section breaks, not mid-paragraph)
  - Preserve markdown formatting for context

**CSV Files (products_catalog.csv):**
- **Strategy:** Each row is a self-contained chunk
- **Metadata:** Include column headers as context for each row
- **Size:** ~200-300 tokens per row (with headers)
- **No overlap needed** (rows are independent)

**FAQ Documents (faq_general_ayurveda_patients.md):**
- **Chunk size:** One Q&A pair per chunk (typically 200-400 tokens)
- **Overlap:** None (Q&A pairs are independent)
- **Metadata:** Question text stored as `question` field, answer as `content`

**Product Dossiers (product_*.md):**
- **Chunk size:** 512 tokens with 128-token overlap
- **Special sections preserved:**
  - "Basic Info" → single chunk
  - "Traditional Positioning" → single chunk
  - "Key Messages" → may span 1-2 chunks
  - "Safety & Precautions" → single chunk (critical for retrieval)

### Chunk Metadata Schema

Each chunk includes:
```json
{
  "doc_id": "product_ashwagandha_tablets_internal.md",
  "chunk_id": "chunk_001",
  "section_path": "Traditional Positioning",
  "chunk_index": 0,
  "total_chunks": 5,
  "doc_type": "product_dossier",
  "tokens": 487,
  "char_start": 0,
  "char_end": 2156
}
```

## 2. Retriever Choice and Rationale

### Hybrid Retrieval: BM25 + Dense Embeddings

**Primary Strategy:** Hybrid retrieval combining:
- **BM25 (sparse):** For exact term matching (product names, herb names, Sanskrit terms)
- **Dense embeddings (sentence-transformers):** For semantic similarity (conceptual queries about stress, digestion, etc.)

### Exact Parameters

**For Short Q&A Queries (< 20 words):**
- **Top-K retrieval:** 5 chunks from dense, 5 chunks from BM25
- **Deduplication:** Remove chunks with >80% token overlap
- **Final set:** 5-8 unique chunks

**For Long Article Generation Queries:**
- **Top-K retrieval:** 10 chunks from dense, 10 chunks from BM25
- **Deduplication:** Remove chunks with >80% token overlap
- **Final set:** 12-15 unique chunks

**Embedding Model:** `sentence-transformers/all-MiniLM-L6-v2` (384 dimensions)

**BM25 Parameters:**
- **k1:** 1.5 (term frequency saturation)
- **b:** 0.75 (length normalization)
- **Fields:** Title (boost 2.0), Content (boost 1.0), Tags (boost 1.5)

**Hybrid Scoring:**
```
final_score = 0.4 * normalized_bm25_score + 0.6 * normalized_embedding_score
```

## 3. Cursor-Based Retrieval Plan

### Streaming Scan Strategy

**Purpose:** Use cursor-based retrieval for fact-checking and incremental evidence gathering.

**Implementation:**
1. **Initial retrieval:** Get top-K chunks via hybrid search
2. **Cursor scan:** For each factual claim in draft answer, scan corpus with cursor
3. **Batch size:** Process 10 chunks per batch
4. **Early-stop heuristics:**
   - Stop if 3+ chunks support the claim (high confidence)
   - Stop if cursor has scanned 50 chunks without support (likely unsupported)
   - Stop if cursor reaches end of relevant document section

### Fact-Checking Workflow

```
1. Generate draft answer from initial retrieval
2. Extract candidate factual sentences (using simple NLP: subject-verb-object patterns)
3. For each sentence:
   a. Extract key entities (product names, herb names, conditions)
   b. Cursor scan: search corpus for these entities + context
   c. Retrieve top-10 chunks for this specific claim
   d. If max similarity < 0.6 threshold → flag as "UNSUPPORTED"
4. Re-generate answer with unsupported claims removed or marked
```

## 4. Prompt Templates

### Template 1: Q&A (Concise Answers)

```
You are a helpful assistant for Kerala Ayurveda, providing accurate information based solely on the provided context documents.

**Context Documents:**
{retrieved_chunks}

**User Query:**
{user_query}

**Instructions:**
1. Answer the query using ONLY information from the context documents above.
2. If the information is not present in the context, respond: "I don't find this in the provided corpus."
3. Include inline citations in square brackets: [source:filename.md#Section Name]
4. Use warm, reassuring, and grounded language (avoid "miracle cure" or "guaranteed" claims).
5. For safety information, be explicit and conservative.
6. Keep the answer concise (2-4 paragraphs for short queries).

**Answer:**
```

### Template 2: Generation/Authoring (Long-Form Content)

```
You are a content writer for Kerala Ayurveda, creating educational articles based on the provided context documents.

**Context Documents:**
{retrieved_chunks}

**Content Request:**
{content_request}

**Instructions:**
1. Use ONLY information from the context documents. Do not invent facts.
2. Structure the article with clear H2/H3 headings.
3. Use short paragraphs (2-4 sentences).
4. Include bulleted lists for practical points.
5. Add inline citations: [source:filename.md#Section Name]
6. Follow the brand voice: warm, reassuring, grounded, tradition-aware, science-friendly.
7. Avoid: disease-cure claims, specific dosing, "guaranteed results."
8. Include a safety note if discussing herbs or therapies.
9. If information is missing, mark it: "MISSING: [description] - requires clinical/editor review."

**Article:**
```

## 5. Citation Format and UI Behavior

### Inline Citation Format

**Pattern:** `[source:doc_id#section_name]`

**Examples:**
- `[source:product_ashwagandha_tablets_internal.md#Traditional Positioning]`
- `[source:faq_general_ayurveda_patients.md#2. How long does it take to see results?]`
- `[source:products_catalog.csv#KA-P001]` (for CSV rows)

### JSON Return Schema

```json
{
  "answer": "Ashwagandha is traditionally used in Ayurveda to support the body's ability to adapt to stress [source:product_ashwagandha_tablets_internal.md#Traditional Positioning]...",
  "citations": [
    {
      "doc_id": "product_ashwagandha_tablets_internal.md",
      "section": "Traditional Positioning",
      "excerpt": "In Ayurveda, Ashwagandha is traditionally used to:\n- Support the body's ability to adapt to stress...",
      "score_note": "dense_similarity: 0.82, bm25_score: 0.45"
    }
  ],
  "unsupported_claims": [],
  "confidence_score": 0.85
}
```

---

# Part A: Code and Examples

## Function Design: `answer_user_query`

The system implements a `KeralaAyurvedaRAG` class with the following key methods:

- `_hybrid_retrieve()`: Combines BM25 and dense embedding retrieval
- `_fact_check_with_cursor()`: Validates factual claims using cursor-based scanning
- `_build_qa_prompt()`: Constructs LLM prompt with retrieved context
- `answer_user_query()`: Main entry point returning answer with citations

See `final_assignment/code/rag_system.py` for full implementation.

## Example Queries

### Example 1: "What are the key benefits of Ashwagandha Stress Balance Tablets? How long until users notice effects?"

**Retrieved Chunks:**
1. `product_ashwagandha_tablets_internal.md#Traditional Positioning`
2. `product_ashwagandha_tablets_internal.md#Key Messages for Content`
3. `faq_general_ayurveda_patients.md#2. How long does it take to see results?`
4. `products_catalog.csv#KA-P002`
5. `product_ashwagandha_tablets_internal.md#Basic Info`

**Sample Answer:**
"Ashwagandha Stress Balance Tablets are designed to support stress resilience and restful sleep [source:product_ashwagandha_tablets_internal.md#Traditional Positioning]. In Ayurveda, Ashwagandha is traditionally used to help the body adapt to stress, promote calmness and emotional balance, and support strength and stamina [source:product_ashwagandha_tablets_internal.md#Traditional Positioning].

The tablets focus on building resilience and adaptation rather than providing sedation [source:product_ashwagandha_tablets_internal.md#Key Messages for Content]. This means they support your body's natural ability to recover and maintain steadiness during challenging times.

Regarding timelines, experiences vary from person to person [source:faq_general_ayurveda_patients.md#2. How long does it take to see results?]. Some people may notice changes in sleep, digestion, or energy within a few weeks, while deeper changes in stress patterns and lifestyle often take longer. We recommend thinking in terms of weeks to months, rather than expecting overnight fixes [source:faq_general_ayurveda_patients.md#2. How long does it take to see results?]."

**Failure Mode:** Query asks for "clinical studies" which is not in corpus. System should respond: "I don't find information about clinical studies or scientific evidence for Ashwagandha Stress Balance Tablets in the provided corpus."

### Example 2: "Are there any contraindications or precautions for Triphala Capsules, especially for people on blood thinners?"

**Retrieved Chunks:**
1. `product_triphala_capsules_internal.md#Safety & Precautions (Content Version)`
2. `products_catalog.csv#KA-P001`
3. `faq_general_ayurveda_patients.md#1. Is Ayurveda safe to combine with modern medicine?`
4. `ayurveda_foundations.md#Kerala Ayurveda Content Boundaries (for writers & agents)`

**Sample Answer:**
"Triphala Capsules are generally considered gentle for digestive support, but there are important precautions to consider [source:product_triphala_capsules_internal.md#Safety & Precautions (Content Version)].

Individuals with chronic digestive disease, recent abdominal surgery, or those who are pregnant or breastfeeding should consult a qualified healthcare provider before using Triphala [source:product_triphala_capsules_internal.md#Safety & Precautions (Content Version)].

Regarding blood thinners specifically, I don't find explicit information about blood thinner interactions in the provided corpus. However, the corpus emphasizes that herb–drug interactions are possible [source:faq_general_ayurveda_patients.md#1. Is Ayurveda safe to combine with modern medicine?]. If you are taking prescription medications, including blood thinners, we strongly recommend consulting both your medical doctor and Ayurvedic practitioner before starting any new supplement [source:faq_general_ayurveda_patients.md#1. Is Ayurveda safe to combine with modern medicine?]."

**Failure Mode:** Query asks for specific blood thinner interaction data not in corpus. System correctly identifies gap and provides general safety guidance.

---

# Part B: Agentic Workflow & Evaluation

## 1. Agentic Workflow (5 Steps)

### Step 1: Query Understanding & Intent Classification
- **Input:** User query, session context
- **Output:** Intent classification, entities, retrieval parameters
- **Failure Mode:** Ambiguous queries cannot be classified
- **Guardrail:** Ask clarifying question if confidence < 0.6

### Step 2: Hybrid Retrieval & Chunk Ranking
- **Input:** Query, intent, retrieval parameters
- **Output:** Ranked list of relevant chunks with scores
- **Failure Mode:** Query terms not in corpus, low-scoring chunks
- **Guardrail:** Return "I don't find this" if max score < 0.3

### Step 3: Answer Generation with Grounding
- **Input:** Query, retrieved chunks, style guide
- **Output:** Draft answer with inline citations
- **Failure Mode:** LLM hallucinates citations or makes unsupported claims
- **Guardrail:** Validate citations match retrieved chunks, remove invalid ones

### Step 4: Cursor-Based Fact-Checking
- **Input:** Draft answer, factual sentences, initial chunks
- **Output:** Fact-check results with supporting evidence
- **Failure Mode:** Cursor misses relevant evidence due to semantic mismatch
- **Guardrail:** Flag claims with similarity < 0.6, require > 0.75 for safety claims

### Step 5: Response Formatting & Citation Enrichment
- **Input:** Verified answer, citations, fact-check results
- **Output:** Final JSON response with enriched citations
- **Failure Mode:** Citation extraction fails to match sections
- **Guardrail:** Use fuzzy matching for section names, log unmatched citations

## 2. Minimal Evaluation Loop

### Golden Set (10 Queries)

1. Product inquiry (Ashwagandha benefits) - Supported
2. Safety question (Triphala + blood thinners) - Partially supported
3. Dosha question (dosha imbalance) - Supported
4. Timeline question (Ashwagandha effects) - Supported
5. Missing information (clinical studies) - Unsupported
6. Combined query (Triphala + pregnancy) - Supported
7. Style/tone check (anxiety cure) - Supported
8. FAQ match (Ayurveda + modern medicine) - Supported
9. Product comparison (Ashwagandha vs Brahmi) - Partially supported
10. General concept (What is Ayurveda?) - Supported

### Scoring Rubric (100 points total)

- **Grounding (40 points):** All claims cited, no unsupported claims
- **Structure (20 points):** Clear paragraphs, proper citations, logical flow
- **Brand Tone (20 points):** Warm, reassuring, no "miracle cure" language
- **Accuracy (20 points):** Information matches corpus exactly

### Metrics to Track

**Daily:**
- Average confidence score (target: >0.75)
- Citation accuracy rate (target: >95%)
- Unsupported claims per answer (target: <0.5)
- Response time p50, p95 (target: p50 <2s, p95 <5s)

**Weekly:**
- Golden set score (target: >85/100 average)
- Safety violation count (target: 0)
- "I don't find this" response rate (target: 5-15%)

## 3. Prioritization for First 2 Weeks

### Week 1: Core Functionality (MUST SHIP)
- Day 1-2: Basic RAG pipeline (hybrid retrieval, answer generation)
- Day 3-4: Safety guardrails (safety-critical detection, enhanced fact-checking)
- Day 5: Citation validation (post-process citations, remove hallucinations)

### Week 2: Quality & Evaluation (MUST SHIP)
- Day 6-7: Style guide enforcement (prompt constraints, tone checker)
- Day 8-9: Evaluation loop (golden set, automated scoring, metrics dashboard)
- Day 10: Cursor-based fact-checking enhancement (true cursor scanning, early-stop)

### Postponed (After Week 2)
- Intent classification (Week 3)
- Advanced cursor strategies (Week 3+)
- Multi-turn conversation (Week 4+)
- User feedback integration (Week 4+)
- Fine-tuned embeddings (Month 2+)

---

# Reflection

## Key Insights

1. **Grounding is Non-Negotiable:** The most critical aspect of this system is ensuring every factual claim is grounded in the corpus. The cursor-based fact-checking step, while computationally expensive, is essential for preventing hallucinations, especially for safety-critical information. The explicit "I don't find this in the provided corpus" response is more valuable than a plausible-sounding but unsupported answer.

2. **Safety Guardrails Must Be Built-In:** Given the medical/health context, safety guardrails cannot be an afterthought. The system needs to detect safety-critical queries (mentions of contraindications, pregnancy, medications) and apply stricter fact-checking thresholds (0.75 vs 0.6). This aligns with the corpus emphasis on consulting healthcare providers, as seen in `ayurveda_foundations.md#Kerala Ayurveda Content Boundaries`.

3. **Hybrid Retrieval Balances Precision and Recall:** BM25 excels at exact term matching (product names, herb names), while dense embeddings capture semantic similarity (conceptual queries). The 40/60 weighting (BM25/dense) was chosen based on the corpus structure: product dossiers have specific terminology (BM25), while general Ayurvedic concepts benefit from semantic search (dense).

4. **Style Guide Enforcement Requires Multi-Layer Approach:** Simply including style guide in the prompt is insufficient. Post-processing is needed to detect and remove discouraged phrases ("miracle cure", "guaranteed"). The corpus's `content_style_and_tone_guide.md` provides clear examples of what to avoid, which can be used for keyword-based filtering.

5. **Evaluation Loop Enables Continuous Improvement:** The 10-query golden set, while small, covers the main query types and edge cases. Daily metrics (confidence, citation accuracy) provide early warning signals, while weekly golden set scoring tracks overall quality. This feedback loop is essential for maintaining quality as the system evolves.

## Challenges Encountered

- **Missing Information Handling:** The corpus lacks specific information (e.g., clinical studies, exact drug interactions). The system must gracefully handle these gaps without hallucinating. The explicit "MISSING" markers and "I don't find this" responses are crucial.

- **Citation Validation:** Ensuring citations match retrieved chunks requires careful post-processing. Section name mismatches (e.g., "Traditional Positioning" vs "Traditional Positioning (Content Version)") require fuzzy matching.

- **Balance Between Completeness and Safety:** The system must provide helpful information while erring on the side of caution for safety questions. This requires careful prompt engineering and fact-checking thresholds.

## Future Improvements

- **Fine-tuned Embeddings:** Training embeddings on Ayurveda-specific text would improve semantic retrieval for domain terms.

- **Intent Classification:** Adding explicit intent classification (product, dosha, safety) would enable more targeted retrieval strategies.

- **Multi-turn Conversation:** Supporting follow-up questions would improve user experience but requires session context management.

- **Advanced Chunking:** More sophisticated chunking (e.g., preserving cross-references, handling tables) would improve retrieval quality.

---

**End of Assignment Submission**


